{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from typing import List, Optional, Union, Tuple # Python >= 3.5\n",
    "from collections import Counter # Python >= 3.1\n",
    "\n",
    "from matplotlib import pyplot as plt # please make sure you have matplotlib installed (pip install matplotlib)\n",
    "import numpy as np # please make sure you have numpy installed (pip install numpy)\n",
    "import pandas as pd # please make sure you have pandas installed (pip install pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exercise 1.1**\n",
    "\n",
    "In this exercise we are going to work with characters in an alphabet.\n",
    "Assume, you have a string `cdd`, then the empirical probability $p(`c`)=\\frac{1}{3}$ and $p(`d`)=\\frac{2}{3}$.\n",
    "Hence, $p$ is a probability distribution over an alphabet.\n",
    "\n",
    "Assume you only have two characters in an alphabet, 'a' and 'b'. \n",
    "\n",
    "\n",
    "* **1. What does the probability of 'a' and 'b' have to be in order for the entropy to be maximal?**\n",
    "* **2. What is the maximum value of the entropy in this case?**\n",
    "* **3. Intuitively, why is the entropy maximum for the value you found?** (You can calculate it on paper or write it in Python, you will need it later anyway)\n",
    "* **Answers** \n",
    "The entropy is given by: $H[p] = -\\sum_xp(x)log_2p(x), \\: \\: x = \\{a,b\\}$\n",
    "* 1. thus, $p(`a`)=p('b') = \\frac{1}{2}$ yields the highest entropy. \n",
    "* 2. $H[p] = -(0.5\\cdot log_2(0.5) + 0.5\\cdot log_2(0.5)) = 1$ bit\n",
    "* 3. Entropy can intuitively be seen as a measure of uncertainty of an RV, and if e.g. $p('a') \\gg p('b')$ we will be more certain that the character $'a'$ will be the value that the RV takes. Thus, if $p('a') = p('b')$, the uncertainty (entropy) will be maximum since we can't say that it is more likely that the RV will take one value over the other. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1.2**\n",
    "\n",
    "Still assume that your alphabet consists of only two characters, 'a' and 'b'.\n",
    "For values $p(`a`) = 0.01, 0.02, \\ldots, 0.99$, plot the entropy of your alphabet ($x$-axis: $p(`a`)$, $y$-axis: Entropy in bits).\n",
    "Your plot should contain only one line.\n",
    "\n",
    "If you don't know how to plot with Matplotlib, there is a really short crash course in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x201c7a00c40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.linspace(0, 2*math.pi, 1000) # generate 1000 x-values between 0 (inclusive) and 2 pi (inclusive)\n",
    "y1 = np.sin(x) # compute sine of x elementwise\n",
    "y2 = np.cos(x) # compute sine of x elementwise\n",
    "plt.plot(x,y1,label=r\"$sin(x)$\") # plot values with label\n",
    "plt.plot(x,y2,label=r\"$cos(x)$\") # plot values with label\n",
    "plt.legend() # show legend\n",
    "# you might need to run plt.show() if nothing shows up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x201c7c76e20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ******************\n",
    "x = np.linspace(0.01, 0.99, 99) # generate 99 x-values between 0.01 (inclusive) and 0.99 (inclusive)\n",
    "Hp = np.zeros(np.shape(x))\n",
    "Hp = [-(x[i]*math.log(x[i], 2) + (1 - x[i])*math.log(1-x[i], 2)) for i in range(len(x))]\n",
    "    \n",
    "plt.plot(x,Hp,label=r\"$H[x]$\") # plot values with label\n",
    "plt.legend()    \n",
    "# ******************\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1.3**\n",
    "\n",
    "A measure that is related to Entropy is the *Gini Impurity* which is defined by\n",
    "$$\n",
    "    G(p) = 1-\\sum_{x\\in\\mathcal{X}}p(x)^2\n",
    "$$\n",
    "\n",
    "Extend the plot from Exercise 1.2, such that you also plot the Gini Impurity for the different values of $p(`a`)$. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x201c7cc62e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ******************\n",
    "x = np.linspace(0.01, 0.99, 99) # generate 99 x-values between 0.01 (inclusive) and 0.99 (inclusive) \n",
    " \n",
    "Hp = [-(x[i]*math.log(x[i], 2) + (1 - x[i])*math.log(1-x[i], 2)) for i in range(len(x))]\n",
    "Gp = [1 - pow(x[i], 2) - pow(1-x[i], 2) for i in range(len(x))]\n",
    "    \n",
    "plt.plot(x,Hp,label=r\"$H[p]$\") # plot values with label\n",
    "plt.plot(x,Gp,label=r\"$G[p]$\") # plot values with label\n",
    "plt.legend() \n",
    "# ******************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "---\n",
    "\n",
    "In the lecture, we talked about a coding scheme with certain information-theoretic properties (Huffman coding). Below you find an implementation of Huffman coding using two classes, `CharacterTree` and `HuffmanCode`. The implementation of `HuffmanCode` might be interesting for you if you need to code a tree-structure in a later assignment. The code for `CharacterTree` is in the file `charactertree.py`.\n",
    "\n",
    "In this exercise, we will work with the Huffman code. Below is a list of methods of this class.\n",
    "\n",
    "* `Huffman(text)`: builds a code tree based on some string `text`.\n",
    "* `unique_characters() -> List[str]`: returns a list of unique characters in the text\n",
    "* `character_code(character: str) -> str`: returns the binary code (as a string) for a character in the code tree. If the character was not in the `text` the tree was built on, an `AttributeError` will be thrown.\n",
    "* `character_probability(character: str) -> float`: returns the probability for a character in the code tree. If the character was not in the `text` the tree was built on, an `AttributeError` will be thrown.\n",
    "* `string_code(string: str) -> str`: calls `character_code` for each character in the code and concatenates the codes to give an enconding of an entire string.\n",
    "* `decode(code: str) -> str`: Decodes an encoded string. The code can be the code for one character or for an entire string.\n",
    "* `entropy() -> float`: Returns the entropy of the code (***not yet implemented***)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charactertree import CharacterTree\n",
    "\n",
    "class HuffmanCode:\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"\n",
    "        :param text: the text should be a string\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self._compute_code_tree()\n",
    "\n",
    "    def unique_characters(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        :return: a list of unique characters in the text\n",
    "        \"\"\"\n",
    "        return list(set(self.text))\n",
    "\n",
    "    def _compute_code_tree(self):\n",
    "        \"\"\"\n",
    "        Compute the code tree for the given text\n",
    "        \n",
    "        :return: nothing but set attribute below\n",
    "        \"\"\"\n",
    "        # use a Counter dictionary for convenience (https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "        counter = Counter(self.text)\n",
    "        # count of all characters (len(self.text) would give the same)\n",
    "        total_character_count = sum(list(counter.values()))\n",
    "        # make list of tuples of format (character, count)\n",
    "        sorted_counter_item_list = list(counter.items())\n",
    "        # build one-element tree for each of the unique words, initialized with their probability (character_count i[1] / total_count)\n",
    "        priority_queue = [CharacterTree(probability=i[1] / total_character_count, character=i[0]) for i in\n",
    "                          sorted_counter_item_list]\n",
    "        # while our tree is not fully there yet (more than one element in the \"queue\")\n",
    "        while len(priority_queue) > 1:\n",
    "            # re-order priority queue according to new probabilities, lowest probability first\n",
    "            priority_queue = sorted(priority_queue, key=lambda e: e.probability)\n",
    "            # take first two elements from the \"queue\"\n",
    "            first, second = priority_queue[:2]\n",
    "            # build new tree with first element as left child and second element as right child\n",
    "            subtree = CharacterTree(probability=first.probability + second.probability, left_child=first,\n",
    "                                    right_child=second)\n",
    "            # add new subtree to priority_queue (without the first two elements that are already in subtree)\n",
    "            priority_queue = [subtree] + priority_queue[2:]\n",
    "        # code tree is the first element once there is only one element left in the queue\n",
    "        self.code_tree = priority_queue[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def _character_code(tree: CharacterTree, character: str, current_code: str, return_probability=False) -> Optional[\n",
    "        Union[str, Tuple[str, float]]]:\n",
    "        \"\"\"\n",
    "        Find the code for a word in the tree.\n",
    "\n",
    "        :param tree: the tree to search in\n",
    "        :param character: the character for which the code should be returned\n",
    "        :param current_code: the current code we already have (because this uses recursion)\n",
    "        :param return_probability: whether we should also return the probability\n",
    "        :return: either None if the character was not found, the code, or the code and the probability if return_probability is set to True\n",
    "        \"\"\"\n",
    "        # go deeper if we are not in a leaf\n",
    "        if tree.character is None:\n",
    "            # recursively check the left subtree\n",
    "            code = HuffmanCode._character_code(tree.left_child, character, current_code + \"0\",\n",
    "                                               return_probability=return_probability)\n",
    "            if code is None:\n",
    "                # recursively check the right subtree if nothing found in the left subtree\n",
    "                code = HuffmanCode._character_code(tree.right_child, character, current_code + \"1\",\n",
    "                                                   return_probability=return_probability)\n",
    "            return code\n",
    "        # else return the code now, exit condition\n",
    "        else:\n",
    "            if tree.character == character:\n",
    "                return current_code if not return_probability else (current_code, tree.probability)\n",
    "            return None\n",
    "\n",
    "    def character_code(self, character: str) -> str:\n",
    "        \"\"\"\n",
    "        Find the code for a character in the tree. Throws an exception if the character is not in the tree.\n",
    "\n",
    "        :param character: the character to find the code for\n",
    "        :return: the code for the character\n",
    "        \"\"\"\n",
    "        code = HuffmanCode._character_code(self.code_tree, character, \"\")\n",
    "        if code is None:\n",
    "            raise AttributeError(f\"Character '{character}' is not part of the code tree.\")\n",
    "        return code\n",
    "\n",
    "    def character_probability(self, character: str) -> float:\n",
    "        \"\"\"\n",
    "        Find the probability of a character in the tree\n",
    "\n",
    "        :param character: the character to find the probability for\n",
    "        :return: the probability of the character\n",
    "        \"\"\"\n",
    "        code_proba = HuffmanCode._character_code(self.code_tree, character, \"\", return_probability=True)\n",
    "        if code_proba is None:\n",
    "            raise AttributeError(\"Character is not part of the code tree\")\n",
    "        return code_proba[1]\n",
    "\n",
    "    def string_code(self, string: str) -> str:\n",
    "        \"\"\"\n",
    "        Find the code for a string in the tree.\n",
    "\n",
    "        :param string: string as a list of\n",
    "        :return: the code for the sentence\n",
    "        \"\"\"\n",
    "        return \"\".join([self.character_code(w) for w in string])\n",
    "\n",
    "    def decode(self, code: str) -> str:\n",
    "        \"\"\"\n",
    "        Decode a coded character or string.\n",
    "\n",
    "        :param code: the code to decode\n",
    "        :return: the decoded string\n",
    "        \"\"\"\n",
    "        # a function for the recursion\n",
    "        def _decode(remaining_code: str, tree: CharacterTree):\n",
    "            if tree.is_leaf():\n",
    "                return remaining_code, tree.character\n",
    "            else:\n",
    "                next_char_zero = remaining_code[0] == \"0\"\n",
    "                return _decode(remaining_code[1:],\n",
    "                               tree.left_child if next_char_zero else tree.right_child)\n",
    "\n",
    "        # start with the whole code\n",
    "        remaining_code = code\n",
    "        # store a list of already decoded words\n",
    "        decode = []\n",
    "        # while there is code left\n",
    "        while len(remaining_code) > 0:\n",
    "            # get the remaining code and the code for the current word\n",
    "            rem_code, word_decode = _decode(remaining_code, self.code_tree)\n",
    "            decode.append(word_decode)\n",
    "            remaining_code = rem_code\n",
    "        return \"\".join(decode)\n",
    "\n",
    "    def entropy(self) -> float:\n",
    "        \"\"\"\n",
    "        The entropy of the alphabet.\n",
    "\n",
    "        :return: the entropy of the alphabet.\n",
    "        \"\"\"\n",
    "        # ***************\n",
    "        #Huffman(text): builds a code tree based on some string text.\n",
    "        #unique_characters() -> List[str]: returns a list of unique characters in the text\n",
    "        #character_code(character: str) -> str: returns the binary code (as a string) for a character in the code tree. If the character was not in the text the tree was built on, an AttributeError will be thrown.\n",
    "        #character_probability(character: str) -> float: returns the probability for a character in the code tree. If the character was not in the text the tree was built on, an AttributeError will be thrown.\n",
    "        #string_code(string: str) -> str: calls character_code for each character in the code and concatenates the codes to give an enconding of an entire string.\n",
    "        #decode(code: str) -> str: Decodes an encoded string. The code can be the code for one character or for an entire string.\n",
    "        #entropy() -> float: Returns the entropy of the code (not yet implemented)\n",
    "        \n",
    "        text_set = self.unique_characters()\n",
    "        prob_list = [self.character_probability(c) for c in text_set]\n",
    "        entropy = -np.dot(prob_list, np.log2(prob_list))\n",
    "        # ***************\n",
    "        return entropy\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.code_tree.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1\n",
    "\n",
    "In the class `HuffmanCode`, there is a method `entropy` that returns the entropy of the encoded alphabet **in bits**.\n",
    "\n",
    "Implement this method and compute the entropy of the following three strings using `HuffmanCode(string1).entropy()`, etc. Note that in this exercise we work with characters not with words as in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\" # Expected entropy: ~4.027\n",
    "string2 = \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\" # Expected entropy: ~4.042\n",
    "string3 = \"Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\" # Expected entropy: ~3.905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string1: 4.027 (expected: 4.027)\n",
      "string2: 4.042 (expected: 4.042)\n",
      "string3: 3.905 (expected: 3.905)\n"
     ]
    }
   ],
   "source": [
    "print(f\"string1: {HuffmanCode(string1).entropy():.3f} (expected: 4.027)\")\n",
    "print(f\"string2: {HuffmanCode(string2).entropy():.3f} (expected: 4.042)\")\n",
    "print(f\"string3: {HuffmanCode(string3).entropy():.3f} (expected: 3.905)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2\n",
    "\n",
    "Considering the alphabet of `string1`, what would be the maximum entropy a text with this alphabet could achieve? Why is this value larger than in Exercise 1? Feel free to find the answer by Python code.\n",
    "\n",
    "$H_{max}('string1') = 4.3923$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hmax = 4.392317422778761\n"
     ]
    }
   ],
   "source": [
    "text_set = HuffmanCode(string1).unique_characters()\n",
    "prob_list = np.full(np.shape(text_set), 1/len(text_set))\n",
    "entropy = -np.sum(np.dot(prob_list, np.log2(prob_list)))\n",
    "print(\"Hmax =\", entropy)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "\n",
    "Solve exercise 1.1 by encoding a string consisting of the characters 'a' and 'b' such that the entropy is maximum.\n",
    "\n",
    "* **How many such strings exist and what properties do they have?** Infinitely many. E.g. 'ab', 'ba', 'aabb', 'bbaa'\n",
    "* **How would a string $s$ look like such that the entropy of $s$ goes to zero?** The entropy would be zero if all characters in the string are the same, e.g. 'aaa'.\n",
    "* **In the lectures we talked about the entropy of a probability distribution whereas now we talk about strings. How are these things related?** The strings will have a probability distribution of its characters and their occurences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4\n",
    "\n",
    "We provide two text files, `lotr_en.txt` and `lotr_de.txt`, that contain text of 'Lord of the Rings (The Fellowship of the Ring)' in English and German respectively. You can create the `HuffmanCode` for both of them as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_lotr_en = HuffmanCode(open(\"lotr_en.txt\", encoding=\"utf-8\").read())\n",
    "code_lotr_de = HuffmanCode(open(\"lotr_de.txt\", encoding=\"utf-8\").read()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one other text file, `test.txt` that contains some other text. Assume we had to encode the text in `test.txt` with the code `code_lotr_en` or `code_lotr_de`. Please make sure to open the file using the UTF-8 encoding `open('test.txt', encoding='utf-8')` as otherwise you can get different values depending on your system's default encoding.\n",
    "\n",
    "* **Without considering `test.txt` for the moment, which language seems to have the lower entropy?** English LOTR entropy:  4.3253\n",
    "* **Measure the cross-entropy to answer the question whether it is more efficient to encode `test.txt` with the English or the German code.** The cross-entropy between 'test.txt' and the English code is lower and thus more efficient than encoding with the German code.\n",
    "* **Why do you think the code you found in the answer to the previous question is better to use?** Most likely because 'test.txt' is in English.\n",
    "* **What would be the best way to encode the text in `test.txt`? Intuitively, why can't the cross-entropy be negative?** By building a character tree using 'test.txt'. The cross-entropy is a measure of $\\textit{extra}$ bits needed to encode e.g. 'test.txt' using the English code. The lowest possible cross-entropy value is $0$, which occurs when the coding tree is built using 'text.txt'. \n",
    "* **How could you easily compute the Kullback-Leibler divergence based on what you already did now?** $H(p,q) = H(p) + D_{KL}(p\\:||\\:q) \\Rightarrow D_{KL}(p\\:||\\:q) = H(p,q) - H(p)$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English LOTR entropy:  4.325265087324359\n",
      "German LOTR entropy:  4.579456199002536\n",
      "Cross entropy test.txt wrt lotr_en:  4.550949133974564\n",
      "Cross entropy test.txt wrt lotr_de:  4.693990976649803\n"
     ]
    }
   ],
   "source": [
    "# *****************\n",
    "entropy_en = code_lotr_en.entropy()\n",
    "entropy_de = code_lotr_de.entropy()\n",
    "print(\"English LOTR entropy: \", entropy_en)\n",
    "print(\"German LOTR entropy: \", entropy_de)\n",
    "\n",
    "code_test = HuffmanCode(open(\"test.txt\", encoding=\"utf-8\").read())  \n",
    "test_uniq = code_test.unique_characters() \n",
    "en_uniq = code_lotr_en.unique_characters()\n",
    "de_uniq = code_lotr_de.unique_characters()   \n",
    "        \n",
    "common_chars_en = [c for c in test_uniq if c in code_lotr_en.unique_characters()]\n",
    "common_chars_de = [c for c in test_uniq if c in code_lotr_de.unique_characters()] \n",
    "  \n",
    "prob_list_test = [code_test.character_probability(c) for c in test_uniq]\n",
    "prob_list_en = [code_lotr_en.character_probability(c) for c in common_chars_en]\n",
    "prob_list_de = [code_lotr_de.character_probability(c) for c in common_chars_de] \n",
    "\n",
    "cross_test_en = -np.sum(np.dot(prob_list_test, np.log2(prob_list_en)))\n",
    "cross_test_de = -np.sum(np.dot(prob_list_test, np.log2(prob_list_de)))\n",
    "\n",
    "print(\"Cross entropy test.txt wrt lotr_en: \", cross_test_en)\n",
    "print(\"Cross entropy test.txt wrt lotr_de: \", cross_test_de) \n",
    "# *****************  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "---\n",
    "\n",
    "In the next exercise, we will work with joint probability distributions. For this purpose, we use the Kaggle Titanic dataset which consists of records of survivors and non-survivors of the Titanic disaster. For this exercise, we will be only interested in some of the attributes, `Survived` and `Sex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Rev. Juozas Montvila</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Miss. Margaret Edith Graham</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Catherine Helen Johnston</td>\n",
       "      <td>female</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. Karl Howell Behr</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Patrick Dooley</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>887 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass                                               Name  \\\n",
       "0           0       3                             Mr. Owen Harris Braund   \n",
       "1           1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2           1       3                              Miss. Laina Heikkinen   \n",
       "3           1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4           0       3                            Mr. William Henry Allen   \n",
       "..        ...     ...                                                ...   \n",
       "882         0       2                               Rev. Juozas Montvila   \n",
       "883         1       1                        Miss. Margaret Edith Graham   \n",
       "884         0       3                     Miss. Catherine Helen Johnston   \n",
       "885         1       1                               Mr. Karl Howell Behr   \n",
       "886         0       3                                 Mr. Patrick Dooley   \n",
       "\n",
       "        Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0      male  22.0                        1                        0   7.2500  \n",
       "1    female  38.0                        1                        0  71.2833  \n",
       "2    female  26.0                        0                        0   7.9250  \n",
       "3    female  35.0                        1                        0  53.1000  \n",
       "4      male  35.0                        0                        0   8.0500  \n",
       "..      ...   ...                      ...                      ...      ...  \n",
       "882    male  27.0                        0                        0  13.0000  \n",
       "883  female  19.0                        0                        0  30.0000  \n",
       "884  female   7.0                        1                        2  23.4500  \n",
       "885    male  26.0                        0                        0  30.0000  \n",
       "886    male  32.0                        0                        0   7.7500  \n",
       "\n",
       "[887 rows x 8 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can compute the probability of a passenger being female or being female and in the third class as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of a female passenger: 0.354\n",
      "Probability of a female passenger and in third class: 0.162\n"
     ]
    }
   ],
   "source": [
    "p_female =  len(titanic.loc[(titanic['Sex'] == \"female\")]) / len(titanic)\n",
    "\n",
    "p_female_and_third_class = len(titanic.loc[(titanic['Sex'] == \"female\") & (titanic['Pclass'] == 3)]) / len(titanic)\n",
    "print(f\"Probability of a female passenger: {p_female:.3f}\")\n",
    "print(f\"Probability of a female passenger and in third class: {p_female_and_third_class:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table gives you the joint a marginal probabilities for the random variables we consider.\n",
    "\n",
    "| $S$ / $V$ | dead | alive | P($S$) |\n",
    "| --------- | ---- | ----- | ------ |\n",
    "| **female** | 0.091 | 0.262 | 0.354 |\n",
    "| **male** | 0.523 | 0.122 | 0.646 |\n",
    "| **P($V$)** | 0.6144 | 0.385 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(S)=0.938\n"
     ]
    }
   ],
   "source": [
    "p_female =  len(titanic.loc[(titanic['Sex'] == \"female\")]) / len(titanic)\n",
    "p_male =  len(titanic.loc[(titanic['Sex'] == \"male\")]) / len(titanic)\n",
    "p_dead = len(titanic.loc[(titanic['Survived'] == 0)]) / len(titanic)\n",
    "p_alive = len(titanic.loc[(titanic['Survived'] == 1)]) / len(titanic)\n",
    "\n",
    "p_female_and_dead = len(titanic.loc[(titanic['Sex'] == \"female\") & (titanic['Survived'] == 0)]) / len(titanic)\n",
    "p_female_and_alive = len(titanic.loc[(titanic['Sex'] == \"female\") & (titanic['Survived'] == 1)]) / len(titanic)\n",
    "p_male_and_dead = len(titanic.loc[(titanic['Sex'] == \"male\") & (titanic['Survived'] == 0)]) / len(titanic) \n",
    "p_male_and_alive = len(titanic.loc[(titanic['Sex'] == \"male\") & (titanic['Survived'] == 1)]) / len(titanic)\n",
    "\n",
    "entropy_sex = - p_female * np.log2(p_female) - p_male * np.log2(p_male)\n",
    "print(f\"H(S)={entropy_sex:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3.2\n",
    "\n",
    "Now, let further $V$ the random variable for a passenger's vitality, distributed according to the empirical distribution in our dataset. You can solve this exercise by coding (using the variables defined above) or with a calculator.\n",
    "\n",
    "* **What is $H(V)$?** $H(V) = 0.9619$\n",
    "* **What is $H(S|V)$?** $H(Y|X) = -\\sum_{x\\in \\mathcal{X},y\\in\\mathcal{Y}} p(x,y)\\:log\\frac{p(x,y)}{p(x)}$, $H(S|V) = 0.7207$\n",
    "* **What is the mutual information $I(S; V)$?** $I(S;V) = 0.2168$\n",
    "* **Does knowing $S$ reduce the uncertainty about $V$?** Yes, since a conditionional entropy measures the uncertainty of V that is left when knowing S it is always reduced. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(V) = 0.9619\n",
      "H(S|V) = 0.7207\n",
      "I(S;V) = 0.2168\n"
     ]
    }
   ],
   "source": [
    "# *****************\n",
    "entropy_vitality = - p_dead * np.log2(p_dead) - p_alive * np.log2(p_alive)  \n",
    "print(f\"H(V) = {entropy_vitality:.4f}\")\n",
    "entropy_cond = -(p_female_and_dead * np.log2(p_female_and_dead/p_dead) + p_female_and_alive * np.log2(p_female_and_alive/p_alive) + p_male_and_dead * np.log2(p_male_and_dead/p_dead) + p_male_and_alive * np.log2(p_male_and_alive/p_alive))\n",
    "print(f\"H(S|V) = {entropy_cond:.4f}\")\n",
    "mutual_info = entropy_sex - entropy_cond\n",
    "print(f\"I(S;V) = {mutual_info:.4f}\")\n",
    "# *****************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
